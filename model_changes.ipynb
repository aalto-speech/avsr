{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of model changes\n",
    "\n",
    "The [DAVEnet model](https://github.com/dharwath/DAVEnet-pytorch) (Harwath et al. 2018) had two precursor models:\n",
    "\n",
    "[NIPS 2016 model](https://papers.nips.cc/paper/6186-unsupervised-learning-of-spoken-language-with-visual-context.pdf) and [ACL 2017 model](https://arxiv.org/pdf/1701.07481.pdf)\n",
    "\n",
    "Code for these two models was not published, but they could be recreated using DAVEnet as a basis. This notebook documents the differences found between these three models to help the replicating process.\n",
    "\n",
    "## Comparison table\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;border:none;border-color:#ccc;margin:0px auto;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 9px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 9px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}\n",
    ".tg .tg-waok{font-weight:bold;font-family:Tahoma, Geneva, sans-serif !important;;border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-td0d{font-family:\"Lucida Sans Unicode\", \"Lucida Grande\", sans-serif !important;;text-align:left;vertical-align:top}\n",
    ".tg .tg-j6ou{background-color:#f9f9f9;font-family:\"Lucida Sans Unicode\", \"Lucida Grande\", sans-serif !important;;text-align:left;vertical-align:top}\n",
    "@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;margin: auto 0px;}}</style>\n",
    "<div class=\"tg-wrap\"><table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-waok\">Model part</th>\n",
    "    <th class=\"tg-waok\">NIPS2016 Model</th>\n",
    "    <th class=\"tg-waok\">ACL 2017 Model</th>\n",
    "    <th class=\"tg-waok\">DAVEnet ECCV 2018</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-td0d\">Image input</td>\n",
    "      <td class=\"tg-j6ou\">Subtract VGG mean pixel value (no mention of variance/std) and take a <i>center</i> 224x224 crop.</td>\n",
    "    <td class=\"tg-td0d\">Presumably the same as NIPS.</td>\n",
    "    <td class=\"tg-j6ou\">Resize smallest dimension to 256, take a <i>random</i> 224x224 crop and normalize with global mean and variance.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-td0d\">Image body</td>\n",
    "    <td class=\"tg-j6ou\">A VGG16 with softmax classification layer removed. VGG weights presumed to be fixed (not specified in the paper).</td>\n",
    "    <td class=\"tg-td0d\">The same as NIPS, weights are known to be fixed.</td>\n",
    "    <td class=\"tg-j6ou\">A VGG16 where the last maxpool and all layers after that are replaced with one convolution layer.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-td0d\">Image output</td>\n",
    "    <td class=\"tg-j6ou\">Linear transform of the 4096 inputs from the penultimate layer to 1024 dimensions.</td>\n",
    "    <td class=\"tg-td0d\">The same as NIPS.</td>\n",
    "    <td class=\"tg-j6ou\">A 3 by 3 linear convolution which outputs a 1024 feature map.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-td0d\">Audio input</td>\n",
    "    <td class=\"tg-j6ou\">Log Mel filter bank spectrograms with 40 filters. Spectrogram normalization. Fixed to L frames (=1024/<b>2048</b>, latter is better) using truncation or zero padding.</td>\n",
    "    <td class=\"tg-td0d\">The same as NIPS, but only L=1024 considered.</td>\n",
    "    <td class=\"tg-j6ou\">Similar to NIPS, but with following changes. Samples are padded to the length of the longest caption in a minibatch. Manual spectrogram normalization is replaced by a BatchNorm layer.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-td0d\">Audio body</td>\n",
    "    <td class=\"tg-j6ou\">3 convolution layers with ReLU, 2 maxpools, 1 mean- or maxpool and a L2 normalization.</td>\n",
    "    <td class=\"tg-td0d\">5 convolution layers with ReLU, 3 maxpools, a meanpool and a L2 normalization.</td>\n",
    "    <td class=\"tg-j6ou\">Similar to ACL, but there is BatchNorm layer at the front. Also L2 normalization removed from the end.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-td0d\">Audio output</td>\n",
    "    <td class=\"tg-j6ou\">1024 dimensional activation vector.</td>\n",
    "    <td class=\"tg-td0d\">The same as NIPS.</td>\n",
    "    <td class=\"tg-j6ou\">1024 dimensional feature map. The padding is removed at this stage, individually for each caption</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-td0d\">Parameters</td>\n",
    "      <td class=\"tg-j6ou\">SGD, 50 epochs, <i>constant</i> momentum 0.9, minibatch size 128. Learning rate: 1e-5, geometrical decay by a factor between 2 and 5 every 5 to 10 epochs.</td>\n",
    "    <td class=\"tg-td0d\">The same as NIPS.</td>\n",
    "    <td class=\"tg-j6ou\">The same as NIPS except models converged in less than 150 epoch on average. Learning rate: 1e-3, decay by factor of 10 in every 70 epochs. </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-td0d\">Similarity function</td>\n",
    "    <td class=\"tg-j6ou\">Dot product</td>\n",
    "    <td class=\"tg-td0d\">Dot product</td>\n",
    "      <td class=\"tg-j6ou\">SISA, <b>MISA</b> (best), SIMA</td>\n",
    "  </tr>\n",
    "</table></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For importing the models folder\n",
    "import sys\n",
    "sys.path.append('/m/home/home4/44/virkkua1/unix/PlacesAudio_project/DAVEnet')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as tvmodels\n",
    "from torchsummary import summary\n",
    "\n",
    "import models\n",
    "\n",
    "# The last line prints False, then there's something funny with Cuda toolkit installation. \n",
    "# For example the toolkit might have been updated without updating the driver which\n",
    "# leads to mismatch and errors.\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Davenet(\n",
       "  (batchnorm1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1): Conv2d(1, 128, kernel_size=(40, 1), stride=(1, 1))\n",
       "  (conv2): Conv2d(128, 256, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5))\n",
       "  (conv3): Conv2d(256, 512, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
       "  (conv4): Conv2d(512, 512, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
       "  (conv5): Conv2d(512, 1024, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
       "  (pool): MaxPool2d(kernel_size=(1, 3), stride=(1, 2), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dave_vgg16 = models.VGG16()\n",
    "#dave_vgg16.cuda()\n",
    "\n",
    "standard_vgg16 = tvmodels.vgg16()\n",
    "standard_vgg16.cuda()\n",
    "\n",
    "audio_model = models.Davenet()\n",
    "audio_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm2d-1          [-1, 1, 40, 1024]               2\n",
      "            Conv2d-2         [-1, 128, 1, 1024]           5,248\n",
      "            Conv2d-3         [-1, 256, 1, 1024]         360,704\n",
      "         MaxPool2d-4          [-1, 256, 1, 512]               0\n",
      "            Conv2d-5          [-1, 512, 1, 512]       2,228,736\n",
      "         MaxPool2d-6          [-1, 512, 1, 256]               0\n",
      "            Conv2d-7          [-1, 512, 1, 256]       4,456,960\n",
      "         MaxPool2d-8          [-1, 512, 1, 128]               0\n",
      "            Conv2d-9         [-1, 1024, 1, 128]       8,913,920\n",
      "        MaxPool2d-10          [-1, 1024, 1, 64]               0\n",
      "================================================================\n",
      "Total params: 15,965,570\n",
      "Trainable params: 15,965,570\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.16\n",
      "Forward/backward pass size (MB): 10.31\n",
      "Params size (MB): 60.90\n",
      "Estimated Total Size (MB): 71.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(audio_model, (40, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dave_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(dave_vgg16, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(standard_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.78\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 747.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(standard_vgg16, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIPS model\n",
    "\n",
    "Trying to replicate the NIPS model using the DAVEnet base and the table of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [128, 64, 1, 1024]          12,864\n",
      "         MaxPool2d-2          [128, 64, 1, 512]               0\n",
      "            Conv2d-3         [128, 512, 1, 512]         819,712\n",
      "         MaxPool2d-4         [128, 512, 1, 256]               0\n",
      "            Conv2d-5        [128, 1024, 1, 256]      13,108,224\n",
      "         MaxPool2d-6          [128, 1024, 1, 1]               0\n",
      "            Linear-7                 [128, 205]         210,125\n",
      "================================================================\n",
      "Total params: 14,150,925\n",
      "Trainable params: 14,150,925\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 20.00\n",
      "Forward/backward pass size (MB): 737.20\n",
      "Params size (MB): 53.98\n",
      "Estimated Total Size (MB): 811.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvX3AudioNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ConvX3AudioNet, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(40,5), stride=(1,1), padding=(0,2))\n",
    "        self.conv2 = nn.Conv2d(64, 512, kernel_size=(1,25), stride=(1,1), padding=(0,12))\n",
    "        self.conv3 = nn.Conv2d(512, 1024, kernel_size=(1,25), stride=(1,1), padding=(0,12))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,4), stride=(1,2), padding=(0,1))\n",
    "        self.globalPool = nn.MaxPool2d(kernel_size=(1,256), stride=(1,1), padding=(0,0))\n",
    "        self.fc = nn.Linear(1024, 205)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.normalize(self.globalPool(x), dim=1)\n",
    "        x = x.squeeze()\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "convX3 = ConvX3AudioNet()\n",
    "convX3.cuda()\n",
    "summary(convX3, (40, 1024), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /var/cache/user/virkkua1/torch/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:14<00:00, 37.2MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 64, 224, 224]           1,792\n",
      "              ReLU-2        [128, 64, 224, 224]               0\n",
      "            Conv2d-3        [128, 64, 224, 224]          36,928\n",
      "              ReLU-4        [128, 64, 224, 224]               0\n",
      "         MaxPool2d-5        [128, 64, 112, 112]               0\n",
      "            Conv2d-6       [128, 128, 112, 112]          73,856\n",
      "              ReLU-7       [128, 128, 112, 112]               0\n",
      "            Conv2d-8       [128, 128, 112, 112]         147,584\n",
      "              ReLU-9       [128, 128, 112, 112]               0\n",
      "        MaxPool2d-10         [128, 128, 56, 56]               0\n",
      "           Conv2d-11         [128, 256, 56, 56]         295,168\n",
      "             ReLU-12         [128, 256, 56, 56]               0\n",
      "           Conv2d-13         [128, 256, 56, 56]         590,080\n",
      "             ReLU-14         [128, 256, 56, 56]               0\n",
      "           Conv2d-15         [128, 256, 56, 56]         590,080\n",
      "             ReLU-16         [128, 256, 56, 56]               0\n",
      "        MaxPool2d-17         [128, 256, 28, 28]               0\n",
      "           Conv2d-18         [128, 512, 28, 28]       1,180,160\n",
      "             ReLU-19         [128, 512, 28, 28]               0\n",
      "           Conv2d-20         [128, 512, 28, 28]       2,359,808\n",
      "             ReLU-21         [128, 512, 28, 28]               0\n",
      "           Conv2d-22         [128, 512, 28, 28]       2,359,808\n",
      "             ReLU-23         [128, 512, 28, 28]               0\n",
      "        MaxPool2d-24         [128, 512, 14, 14]               0\n",
      "           Conv2d-25         [128, 512, 14, 14]       2,359,808\n",
      "             ReLU-26         [128, 512, 14, 14]               0\n",
      "           Conv2d-27         [128, 512, 14, 14]       2,359,808\n",
      "             ReLU-28         [128, 512, 14, 14]               0\n",
      "           Conv2d-29         [128, 512, 14, 14]       2,359,808\n",
      "             ReLU-30         [128, 512, 14, 14]               0\n",
      "        MaxPool2d-31           [128, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32           [128, 512, 7, 7]               0\n",
      "           Linear-33                [128, 4096]     102,764,544\n",
      "             ReLU-34                [128, 4096]               0\n",
      "          Dropout-35                [128, 4096]               0\n",
      "           Linear-36                [128, 4096]      16,781,312\n",
      "             ReLU-37                [128, 4096]               0\n",
      "          Dropout-38                [128, 4096]               0\n",
      "           Linear-39                [128, 1024]       4,195,328\n",
      "              VGG-40                [128, 1024]               0\n",
      "================================================================\n",
      "Total params: 138,455,872\n",
      "Trainable params: 4,195,328\n",
      "Non-trainable params: 134,260,544\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 73.50\n",
      "Forward/backward pass size (MB): 28005.00\n",
      "Params size (MB): 528.17\n",
      "Estimated Total Size (MB): 28606.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class VGG16withFC(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(VGG16withFC, self).__init__()\n",
    "        seed_model = tvmodels.__dict__['vgg16'](pretrained=True)\n",
    "        # Remove last 1000-dim class transform\n",
    "        seed_model.classifier = nn.Sequential(*list(seed_model.classifier.children())[:-1])\n",
    "        # Freeze params\n",
    "        for param in seed_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Add a linear transform of the embedding dimension size\n",
    "        last_layer_index = len(list(seed_model.classifier.children()))\n",
    "        seed_model.classifier.add_module(str(last_layer_index),\n",
    "                                         nn.Linear(4096, embedding_dim))\n",
    "        self.image_model = seed_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.image_model(x)\n",
    "        return x\n",
    "\n",
    "vgg16withFC = VGG16withFC()\n",
    "vgg16withFC.cuda()\n",
    "summary(vgg16withFC, (3, 224, 224), batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
